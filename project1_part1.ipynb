{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f842a40-f75b-46d9-8ed3-62a4fca79551",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import avg ,count,col,lit, expr,dayofweek,to_date,desc\n",
    "from pyspark.sql.functions import sum as sums\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"my project 1\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Read a CSV into a dataframe\n",
    "# There is a smarter version, that will first check if there is a Parquet file and use it\n",
    "def load_PD_file(filename_or_dir, schema) :\n",
    "    dataPath = \"/mnt/ddscoursedatastorage/fwm-stb-data/\" + filename_or_dir\n",
    "    df = spark.read.format(\"csv\")\\\n",
    "      .option(\"header\",\"false\")\\\n",
    "      .option(\"delimiter\", \"|\")\\\n",
    "      .schema(schema)\\\n",
    "      .load(dataPath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f18f520-8dfa-4b97-aeb5-1032f3f9e837",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv(\"/mnt/ddscoursedatastorage/dds-students/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa64c107-30c1-4e67-ad63-99fdb8d60b0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203581233\n+------------+------+--------+------------+--------------+-----------+-------+\n|   device_id|   dma|dma_code|household_id|household_type|system_type|zipcode|\n+------------+------+--------+------------+--------------+-----------+-------+\n|00000008354e|Toledo|     547|     1522829|        FWM-ID|          H|  43434|\n|0000009ef08e|Toledo|     547|     1522829|        FWM-ID|          H|  43434|\n|0000037bb803|Toledo|     547|     1522829|        FWM-ID|          H|  43434|\n|0000006610c1|Toledo|     547|     1438798|        FWM-ID|          H|  43460|\n|00000066807b|Toledo|     547|     1438798|        FWM-ID|          H|  43460|\n+------------+------+--------+------------+--------------+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading the Reference Parquet files\n",
    "\n",
    "ref_data = spark.read.parquet('/ref_data_raw').withColumnRenamed(\"_device-id\",\"device_id\")\\\n",
    "                                                .withColumnRenamed(\"_dma\",\"dma\")\\\n",
    "                                                .withColumnRenamed(\"_dma-code\",\"dma_code\")\\\n",
    "                                                .withColumnRenamed(\"_household-id\",\"household_id\")\\\n",
    "                                                .withColumnRenamed(\"_household-type\",\"household_type\")\\\n",
    "                                                .withColumnRenamed(\"_system-type\",\"system_type\")\\\n",
    "                                                .withColumnRenamed(\"_zipcode\",\"zipcode\")\n",
    "ref_data_count = ref_data.count()\n",
    "print(ref_data_count)\n",
    "ref_data.limit(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b9cd6e4-f3d0-4272-a054-2578ab52849c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-----------+--------+--------+--------+\n|     prog_code|         title|      genre|air_date|air_time|Duration|\n+--------------+--------------+-----------+--------+--------+--------+\n|EP000000250035|21 Jump Street|Crime drama|20151219|  050000|    60.0|\n|EP000000250035|21 Jump Street|Crime drama|20151219|  110000|    60.0|\n|EP000000250063|21 Jump Street|Crime drama|20151219|  180000|    60.0|\n+--------------+--------------+-----------+--------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading the Daily Programs CSV file\n",
    "\n",
    "daily_prog_schema =  StructType([StructField('prog_code',StringType()),\n",
    "                     StructField('title',StringType()),\n",
    "                     StructField('genre',StringType()),\n",
    "                     StructField('air_date',StringType()),\n",
    "                     StructField('air_time',StringType()),\n",
    "                     StructField('Duration',FloatType())\n",
    "                                       ])\n",
    "daily_prog_data = load_PD_file(\"Daily program data/\" , daily_prog_schema )\n",
    "\n",
    "daily_prog_data.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac496406-b0a7-4f09-a070-ad4a64638c6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 130,289,194 entries in viewing_data dataframe!\n"
     ]
    }
   ],
   "source": [
    " #Reading the 2.5% sample of the viewing data from a Parquet file\n",
    " \n",
    "viewing_data = spark.read.parquet('/sample_viewing_2_5percent')\n",
    " \n",
    "print(f'There are {viewing_data.count():,} entries in viewing_data dataframe!')\n",
    "\n",
    "# selected_viewing_data_df=viewing_data.select(\"device_id\",\"event_date\",\"event_time\",\"prog_code\").dropDuplicates()\n",
    "#selected_viewing_data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4a99790-4a22-4b5a-b265-f2902dbbcac8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the Demographic CSV file\n",
    "\n",
    "demographic_schema =  StructType([StructField('household_id',StringType()),\n",
    "                      StructField('household_size',IntegerType()),\n",
    "                      StructField('num_adults',IntegerType()),\n",
    "                      StructField('num_generations',IntegerType()),\n",
    "                      StructField('adult_range',StringType()),\n",
    "                      StructField('marital_status',StringType()),\n",
    "                      StructField('race_code',StringType()),\n",
    "                      StructField('presence_children',StringType()),\n",
    "                      StructField('num_children',IntegerType()),\n",
    "                      StructField('age_children',StringType()), #format like range - 'bitwise'\n",
    "                      StructField('age_range_children',StringType()),\n",
    "                      StructField('dwelling_type',StringType()),\n",
    "                      StructField('home_owner_status',StringType()),\n",
    "                      StructField('length_residence',IntegerType()),\n",
    "                      StructField('home_market_value',StringType()),\n",
    "                      StructField('num_vehicles',IntegerType()),\n",
    "                      StructField('vehicle_make',StringType()),\n",
    "                      StructField('vehicle_model',StringType()),\n",
    "                      StructField('vehicle_year',IntegerType()),\n",
    "                      StructField('net_worth',IntegerType()),\n",
    "                      StructField('income',StringType()),\n",
    "                      StructField('gender_individual',StringType()),\n",
    "                      StructField('age_individual',IntegerType()),\n",
    "                      StructField('education_highest',StringType()),\n",
    "                      StructField('occupation_highest',StringType()),\n",
    "                      StructField('education_1',StringType()),\n",
    "                      StructField('occupation_1',StringType()),\n",
    "                      StructField('age_2',IntegerType()),\n",
    "                      StructField('education_2',StringType()),\n",
    "                      StructField('occupation_2',StringType()),\n",
    "                      StructField('age_3',IntegerType()),\n",
    "                      StructField('education_3',StringType()),\n",
    "                      StructField('occupation_3',StringType()),\n",
    "                      StructField('age_4',IntegerType()),\n",
    "                      StructField('education_4',StringType()),\n",
    "                      StructField('occupation_4',StringType()),\n",
    "                      StructField('age_5',IntegerType()),\n",
    "                      StructField('education_5',StringType()),\n",
    "                      StructField('occupation_5',StringType()),\n",
    "                      StructField('polit_party_regist',StringType()),\n",
    "                      StructField('polit_party_input',StringType()),\n",
    "                      StructField('household_clusters',StringType()),\n",
    "                      StructField('insurance_groups',StringType()),\n",
    "                      StructField('financial_groups',StringType()),\n",
    "                      StructField('green_living',StringType())\n",
    "                                       ])\n",
    "\n",
    "demographic_data = load_PD_file(\"demographic/\" , demographic_schema  )  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e57611a-dca9-42c4-ac1f-7c84e3d19254",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: [Row(income='7'),\n Row(income='3'),\n Row(income='8'),\n Row(income=None),\n Row(income='5'),\n Row(income='B'),\n Row(income='6'),\n Row(income='D'),\n Row(income='C'),\n Row(income='A'),\n Row(income='9'),\n Row(income='1'),\n Row(income='4'),\n Row(income='2')]"
     ]
    }
   ],
   "source": [
    "\n",
    "#select_only_relevant columns\n",
    "selected_daily_prog_data_df=daily_prog_data.select(\"prog_code\",\"title\",\"genre\",\"air_date\",\"air_time\",\"Duration\").distinct()\n",
    "selected_demographic_data_df=demographic_data.select(\"household_id\",\"household_size\",\"num_adults\",\"net_worth\",\"income\").distinct()\n",
    "\n",
    "selected_viewing_data_df=viewing_data.select(\"device_id\",\"event_date\",\"event_time\",\"prog_code\").dropDuplicates()\n",
    "\n",
    "selected_ref_df=ref_data.select(\"device_id\",\"DMA\",\"dma_code\",\"household_id\").na.drop().dropDuplicates()\n",
    "\n",
    "selected_demographic_data_df.select(\"income\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce83250-2ac1-41a4-a71b-1ac7d624b45e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+----------+---------+------+\n|household_id|household_size|num_adults|net_worth|income|\n+------------+--------------+----------+---------+------+\n|    00000122|             3|         2|        7|     5|\n|    00000109|             1|         1|     null|  null|\n|    00000026|          null|      null|     null|  null|\n|    00000117|          null|      null|     null|  null|\n|    00000099|          null|      null|     null|  null|\n|    00000035|             1|         1|        4|  null|\n|    00000126|             2|         2|        6|  null|\n|    00000085|             2|         1|        5|  null|\n|    00000048|          null|      null|     null|  null|\n|    00000040|             2|         2|        4|     5|\n|    00000056|             2|         2|        5|  null|\n|    00000111|             2|         2|        8|     4|\n|    00000024|             2|         2|        7|     7|\n|    00000098|             3|         2|        7|     8|\n|    00000042|          null|      null|     null|  null|\n|    00000028|             3|         2|        5|     7|\n|    00000129|          null|      null|     null|  null|\n|    00000015|             2|         2|        6|     4|\n|    00000120|             1|         1|     null|  null|\n|    00000036|          null|      null|     null|  null|\n+------------+--------------+----------+---------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "#Convert the values of the Letters A-D to 10-13 and convert income to integer and drop nulls\n",
    "selected_demographic_data_df2=selected_demographic_data_df\n",
    "selected_demographic_data_df2 = selected_demographic_data_df2.withColumn('income', when(selected_demographic_data_df.income == 'A', 10).otherwise(selected_demographic_data_df.income))\n",
    "selected_demographic_data_df2 = selected_demographic_data_df2.withColumn('income', when(selected_demographic_data_df2.income == 'B', 11).otherwise(selected_demographic_data_df2.income))\n",
    "selected_demographic_data_df2 = selected_demographic_data_df2.withColumn('income', when(selected_demographic_data_df2.income == 'C', 12).otherwise(selected_demographic_data_df2.income))\n",
    "selected_demographic_data_df2 = selected_demographic_data_df2.withColumn('income', when(selected_demographic_data_df2.income == 'D', 13).otherwise(selected_demographic_data_df2.income))\n",
    "#Drop nulls\n",
    "selected_demographic_data_df2 = selected_demographic_data_df2.withColumn('income', col('income').cast('integer'))\n",
    "selected_demographic_data_df2 = selected_demographic_data_df2.na.drop(subset=['income'])\n",
    "selected_demographic_data_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "494e2407-a57c-4aa3-bc57-f15ce324e760",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, avg\n",
    "# Calculate the average income\n",
    "average_income = selected_demographic_data_df2.na.drop().select(avg(col(\"income\"))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "147606b2-b93a-4508-842d-bf35b0339b4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Question 1.1 applaying trasformations, joining the data and adding 3 new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e234dfa-0a0d-4d9a-a039-72b1bbb2adf3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prog_code', 'title', 'genre', 'air_date', 'air_time', 'Duration', 'device_id', 'event_date', 'event_time', 'prog_code', 'device_id', 'DMA', 'dma_code', 'household_id', 'household_id', 'household_size', 'num_adults', 'net_worth', 'income', 'has_bad_genre', 'day_prog', 'less_adult_high_networth']\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#join the whole df's into 1 df and adding 3 columns that will help us to make the queries \n",
    "\n",
    "from pyspark.sql.functions import col, date_format, to_date\n",
    "from pyspark.sql.functions import col, udf, desc,avg\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col, udf, desc, avg\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "#join the whole df's into one df\n",
    "\n",
    "view_daily = selected_daily_prog_data_df.join(selected_viewing_data_df, selected_daily_prog_data_df.prog_code == selected_viewing_data_df.prog_code, \"inner\")\n",
    "join_without_demographic = view_daily.join(selected_ref_df, view_daily.device_id == selected_ref_df.device_id, \"inner\")\n",
    "full_join = join_without_demographic.join(selected_demographic_data_df2, join_without_demographic.household_id == selected_demographic_data_df2.household_id, \"inner\")\n",
    "\n",
    "# Define a UDF to check if any bad genre is present and create a column that will count them\n",
    "bad_genres = [\"talk\", \"politics\", \"news\", \"community\", \"crime\"]\n",
    "\n",
    "\n",
    "contains_bad_genre = udf(lambda genre: int(any(g.lower() in genre.lower().split(\",\") for g in bad_genres)), IntegerType())\n",
    "\n",
    "# Replace null values in the genre column with an empty string\n",
    "full_join = full_join.withColumn(\"genre\", when(col(\"genre\").isNull(), \"\").otherwise(col(\"genre\")))\n",
    "\n",
    "# present how many \"bad geners\" each program has\n",
    "full_join = full_join.withColumn(\"has_bad_genre\", contains_bad_genre(col(\"genre\"))).cache()\n",
    "\n",
    "\n",
    "#add column to show in which day of the week the program had been brodcasted\n",
    "full_join = full_join.withColumn(\"day_prog\", to_date(col(\"air_date\"), \"yyyyMMdd\"))\n",
    "full_join=full_join.withColumn(\"day_prog\", date_format(col(\"day_prog\"), \"EEEE\"))\n",
    "\n",
    "\n",
    "#add column that give us indication if the row ias applying condition 3\n",
    "full_join = full_join.withColumn(\"less_adult_high_networth\", when((col(\"num_adults\") <3) &(col(\"net_worth\")>8) , True).otherwise(False))\n",
    "\n",
    "print(full_join.columns)\n",
    "full_join.limit(5).show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36ab2610-1c32-4a13-a67a-e11d087c739a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#question 1.2_2 calculating the queries and printing the number of programs that having the condition of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b9620a-2fc1-4d7e-9675-e4114996a0d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9,138 entries in condition_1 dataframe!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "#query 1: The prog code was viewed by a device with less than 5 average daily events\n",
    "\n",
    "# Register the DataFrame as a temporary view\n",
    "selected_viewing_data_df.createOrReplaceTempView(\"selected_viewing_data_view\")\n",
    "\n",
    "# Execute the SQL query that counts the average daily events fot each unique device\n",
    "avg_daily_events = spark.sql(\"\"\"\n",
    "    SELECT device_id, COUNT(*) / COUNT(DISTINCT event_date) AS avg_daily_events\n",
    "    FROM selected_viewing_data_view\n",
    "    GROUP BY device_id\n",
    "\"\"\")\n",
    "\n",
    "# Join the \"device_id\" and \"prog_code\" columns from \"selected_viewing_data_df\" with \"avg_daily_events\"\n",
    "device_prog = selected_viewing_data_df.select(\"device_id\", \"prog_code\")\n",
    "condition_1 = device_prog.join(avg_daily_events, device_prog[\"device_id\"] == avg_daily_events[\"device_id\"], \"inner\").distinct()\n",
    "condition_1.cache()\n",
    "# Filter rows where \"avg_daily_events\" is less than 5 and count the number of rows\n",
    "condition_1 = condition_1.select(\"prog_code\").filter(col(\"avg_daily_events\") < 5).distinct()\n",
    "q1_num = condition_1.count()\n",
    "print(f'There are {q1_num:,} entries in condition_1 dataframe!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad953e2-6129-49de-b990-87e8445b683f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 235,428 entries in condition_2 dataframe!\n"
     ]
    }
   ],
   "source": [
    "#query 2: The prog code was watched by a device from a DMA name that contains the letter [‘z’] \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "selected_ref_df=selected_ref_df.withColumnRenamed(\"device_id\",\"device_id1\")\n",
    "                                    \n",
    "condition_2=selected_ref_df.join(selected_viewing_data_df, selected_ref_df[\"device_id1\"] == selected_viewing_data_df[\"device_id\"], \"inner\").distinct()\n",
    "#take dma with Z in their DMA name\n",
    "condition_2 = condition_2.filter(col(\"DMA\").like(\"%z%\") | col(\"DMA\").like(\"%Z%\")).select(\"prog_code\").distinct()\n",
    "q2_num=condition_2.count()\n",
    "print(f'There are {q2_num:,} entries in condition_2 dataframe!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb8d7e8-6210-423b-8bdc-61d4d41da77a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 180,061 entries in condition_3 dataframe!\n"
     ]
    }
   ],
   "source": [
    "# query 3 : The program was watched by a family with less than 3 adults and their net worth is higher than 8\n",
    "#Take only houses with less than 3 adults and net-worth larger than 8\n",
    "relevant_houses=selected_demographic_data_df.filter((selected_demographic_data_df.num_adults.cast(\"int\") < 3 )& (selected_demographic_data_df.net_worth.cast(\"int\") > 8))\n",
    "\n",
    "relevant_houses=relevant_houses.withColumnRenamed(\"household_id\",\"household_id1\")\n",
    "#joining with reference data to get the devices in these households\n",
    "relevant_devices = relevant_houses.join(selected_ref_df, relevant_houses[\"household_id1\"] == selected_ref_df[\"household_id\"], \"inner\").distinct()\n",
    "\n",
    "relevant_devices=relevant_devices.select(\"device_id1\").distinct().withColumnRenamed(\"device_id\",\"device_id1\")\n",
    "condition_3=relevant_devices.join(selected_viewing_data_df, relevant_devices[\"device_id1\"] == selected_viewing_data_df[\"device_id\"], \"inner\").distinct()\n",
    "                                      \n",
    "condition_3 = condition_3.select(\"prog_code\").distinct()\n",
    "q3_num = condition_3.count()\n",
    "print(f'There are {q3_num:,} entries in condition_3 dataframe!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40648e3a-4ef5-4416-a3e5-cdfcead87b4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 67,646 entries in condition_4 dataframe!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_format, to_date\n",
    "\n",
    "# #query 4: The program code was aired (at least once) between Friday at 6PM and Saturday at 7PM (both\n",
    "# inclusive) and there was at-least one household who watched the program with size higher than or\n",
    "# equal to 8 (inclusive).\n",
    "\n",
    "\n",
    "\n",
    "#Add column of day_prog its the day in the week as a string \n",
    "selected_viewing_data_df_edited = selected_daily_prog_data_df.withColumn(\"day_prog\", to_date(col(\"air_date\"), \"yyyyMMdd\"))\n",
    "selected_viewing_data_df_edited=selected_viewing_data_df_edited.withColumn(\"day_prog\", date_format(col(\"day_prog\"), \"EEEE\"))\n",
    "selected_viewing_data_df_edited = selected_viewing_data_df_edited.withColumn(\"air_time\", col(\"air_time\").cast(\"integer\"))\n",
    "\n",
    "\n",
    "#Filter based on air time between Friday 6PM and Saturday 7PM\n",
    "relevant_programs_days = selected_viewing_data_df_edited.filter( ((col(\"day_prog\") == \"Friday\") & (col(\"air_time\") >= 180000)) |\n",
    "    ((col(\"day_prog\") == \"Saturday\") & (col(\"air_time\") <= 190000))\n",
    ").select(\"prog_code\", \"air_date\", \"air_time\").withColumnRenamed(\"prog_code\", \"prog_code1\").distinct()\n",
    "\n",
    "\n",
    "\n",
    "relevant_programs_days = selected_viewing_data_df.join(\n",
    "    relevant_programs_days,\n",
    "    (selected_viewing_data_df[\"prog_code\"] == relevant_programs_days[\"prog_code1\"]) ,\n",
    "   \n",
    "    \"inner\"\n",
    ").select(\"device_id\", \"prog_code\").distinct()\n",
    "\n",
    "\n",
    "# Join the selected_ref_df DataFrame with relevant_programs_days DataFrame based on the \"device_id1\" column\n",
    "# Using an inner join and selecting distinct rows\n",
    "join2 = selected_ref_df.join(relevant_programs_days, selected_ref_df[\"device_id1\"] == relevant_programs_days[\"device_id\"], \"inner\").distinct()\n",
    "\n",
    "# Drop rows with null values in the \"household_size\" column of selected_demographic_data_df DataFrame\n",
    "selected_demographic_data_df = selected_demographic_data_df.na.drop(subset=[\"household_size\"])\n",
    "\n",
    "# Filter selected_demographic_data_df to get relevant houses with household_size greater than or equal to 8\n",
    "relevant_houses = selected_demographic_data_df.filter(col(\"household_size\") >= 8).select(\"household_id\").distinct().withColumnRenamed(\"household_id\", \"household_id1\")\n",
    "\n",
    "# Join join2 DataFrame with relevant_houses DataFrame based on the \"household_id\" column\n",
    "# Using an inner join and selecting the \"prog_code\" column\n",
    "condition_4 = join2.join(relevant_houses, join2[\"household_id\"] == relevant_houses[\"household_id1\"], \"inner\").select(\"prog_code\").distinct()\n",
    "\n",
    "# Cache the condition_4 DataFrame for better performance\n",
    "condition_4.cache()\n",
    "\n",
    "# Count the distinct values in the \"prog_code\" column of the condition_4 DataFrame\n",
    "condition_4 = condition_4.select(\"prog_code\").distinct()\n",
    "q4_num = condition_4.count()\n",
    "# Print the value of q4_num\n",
    "print(f'There are {q4_num:,} entries in condition_4 dataframe!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19cb72d2-9fd7-4d5d-913e-1dced7858095",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: 6.715162771873656"
     ]
    }
   ],
   "source": [
    "selected_demographic_data_without_null = selected_demographic_data_df2.na.drop(subset=[\"income\"])\n",
    "\n",
    "# Calculate the average income for query 5\n",
    "average_income = selected_demographic_data_without_null.select(avg(\"income\")).first()[0]\n",
    "average_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6370c67c-94d2-4ae0-bbcf-dd09ed6674b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 255,489 entries in condition_5 dataframe!\n"
     ]
    }
   ],
   "source": [
    "#query 5 :The prog code was watched (at least once) by a device from an household with more than 3 devices\n",
    "# (exclusive) and the income of that household is lower than the average household income in the\n",
    "# data.\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, countDistinct\n",
    "\n",
    "#Drop the nulls from income column\n",
    "selected_demographic_data_without_null = selected_demographic_data_df2.na.drop(subset=[\"income\"])\n",
    "\n",
    "# Calculate the average income\n",
    "average_income = selected_demographic_data_without_null.select(avg(\"income\")).first()[0]\n",
    "HighIncome=selected_demographic_data_without_null.filter((col(\"income\")<average_income)).select(\"household_id\").withColumnRenamed(\"household_id\",\"household_id1\")\n",
    "\n",
    "# Rename the \"household_id\" column in selected_ref_df DataFrame to \"household_id2\"\n",
    "ref_df=selected_ref_df.withColumnRenamed(\"household_id\",\"household_id2\")\n",
    "# Join ref_df DataFrame with HighIncome DataFrame based on the \"household_id2\" column using an inner join\n",
    "DeviceHousehold=ref_df.join(HighIncome, ref_df[\"household_id2\"]==HighIncome[\"household_id1\"], \"inner\")\n",
    "\n",
    "\n",
    "# Select the \"household_id1\" and \"device_id1\" columns from DeviceHousehold DataFrame and drop the \"household_id2\" column\n",
    "DeviceHousehold = DeviceHousehold.select(\"household_id1\", \"device_id1\").drop(\"household_id2\")\n",
    "\n",
    "# Group the DeviceHousehold DataFrame by \"household_id1\" and compute the count of distinct values in the \"device_id1\" column # Rename the result column as \"sum_value\" which counts the devices of the household  and filter\n",
    "#the result where it has more than 3 devices \n",
    "DeviceHousehold=DeviceHousehold.groupBy('household_id1').agg(countDistinct('device_id1').alias('sum_value')).filter(col(\"sum_value\")>3).withColumnRenamed(\"device_id\", \"device_id1\")\n",
    "\n",
    "\n",
    "# Join the DeviceHousehold DataFrame with selected_ref_df DataFrame based on the \"household_id\" column using an inner join\n",
    "DeviceHousehold = DeviceHousehold.join(selected_ref_df, selected_ref_df[\"household_id\"] == DeviceHousehold[\"household_id1\"], \"inner\")\n",
    "\n",
    "# Select the \"household_id1\" and \"device_id1\" columns from the DeviceHousehold DataFrame, drop duplicates, and rename \"device_id1\" as \"device_id\"\n",
    "DeviceHousehold = DeviceHousehold.select(\"household_id1\", \"device_id1\").dropDuplicates().withColumnRenamed(\"device_id1\", \"device_id\")\n",
    "\n",
    "# Join the selected_viewing_data_df DataFrame with the DeviceHousehold DataFrame based on the \"device_id\" column using an inner join\n",
    "condition_5 = selected_viewing_data_df.join(DeviceHousehold, selected_viewing_data_df[\"device_id\"] == DeviceHousehold[\"device_id\"], \"inner\").select(\"prog_code\").distinct()\n",
    "\n",
    "# Count the distinct values in the \"prog_code\" column of the condition_5 DataFrame\n",
    "q5_num = condition_5.count()\n",
    "\n",
    "# Print the value of query_5_num\n",
    "print(f'There are {q5_num:,} entries in condition_5 dataframe!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03204681-8832-4ca5-9753-01cc9473e831",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33,648 entries in condition_6 dataframe!\n"
     ]
    }
   ],
   "source": [
    "# query 6 :• The program contains at least one of the genres [‘Talk’, ‘Politics’, ‘News’, ‘Community’, ‘Crime’]\n",
    "# and has a duration of more than 35 minutes (exclusive).\n",
    "from pyspark.sql.functions import col, udf, desc, avg\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "bad_genres = [\"talk\", \"politics\", \"news\", \"community\", \"crime\"]\n",
    "\n",
    "# Define a UDF to check if any bad genre is present\n",
    "contains_bad_genre = udf(lambda genre: int(any(g.lower() in genre.lower().split(\",\") for g in bad_genres)), IntegerType())\n",
    "\n",
    "# Replace null values in the genre column with an empty string\n",
    "query_6 = selected_daily_prog_data_df.withColumn(\"genre\", when(col(\"genre\").isNull(), \"\").otherwise(col(\"genre\")))\n",
    "\n",
    "#add a column has_bad_genre and the method contains_bad_genre will split the genres and add 1 if it contains this genre else 0\n",
    "query_6 = query_6.withColumn(\"has_bad_genre\", contains_bad_genre(col(\"genre\")))\n",
    "\n",
    "condition_6 = query_6.filter((col(\"has_bad_genre\")>0)&(col(\"Duration\")>35)).select(\"prog_code\").distinct()\n",
    "q6_num=condition_6.count()\n",
    "print(f'There are {q6_num:,} entries in condition_6 dataframe!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba7e1e53-ef46-4064-b94d-7fc7eba6b808",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n|     prog_code|condition_count|\n+--------------+---------------+\n|     DVRPGMKEY|              4|\n|EP000000211576|              4|\n|EP000000211614|              4|\n|EP000000211639|              4|\n|EP000000211645|              4|\n|EP000000211646|              5|\n|EP000000211647|              5|\n|EP000000211648|              5|\n|EP000000211650|              4|\n|EP000000211654|              4|\n|EP000000211659|              4|\n|EP000000211661|              4|\n|EP000000211665|              4|\n|EP000000211666|              4|\n|EP000000211667|              4|\n|EP000000211669|              5|\n|EP000000211670|              4|\n|EP000000211672|              5|\n|EP000000211675|              4|\n|EP000000211676|              4|\n+--------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, sum\n",
    "#we are assuming that there are might be prog_codes in viewing data that does not appear in daily_prog data so there might be diffrent ressult if we take diffrent data frames in our\n",
    "#conditions, further more we understad that the assigment asks us to print only the prog_codes\n",
    "# Create a new column to count the number of conditions that apply to each record\n",
    "condition_1=condition_1.select(\"prog_code\").withColumn(\"condition\",lit(1)).distinct()\n",
    "condition_2=condition_2.select(\"prog_code\").withColumn(\"condition\",lit(1)).distinct()                                  \n",
    "condition_3=condition_3.select(\"prog_code\").withColumn(\"condition\",lit(1)).distinct()                                     \n",
    "condition_4=condition_4.select(\"prog_code\").withColumn(\"condition\",lit(1)).distinct()                                     \n",
    "condition_5=condition_5.select(\"prog_code\").withColumn(\"condition\",lit(1)).distinct() \n",
    "condition_6=condition_6.select(\"prog_code\").withColumn(\"condition\",lit(1)).distinct()                                 \n",
    "                                  \n",
    "condition_1.cache()\n",
    "condition_2.cache()\n",
    "condition_3.cache()\n",
    "condition_4.cache()\n",
    "condition_5.cache()\n",
    "condition_6.cache()\n",
    "\n",
    "\n",
    "# Combine all conditions using union\n",
    "combined_conditions = condition_1.union(condition_2).union(condition_3).union(condition_4).union(condition_5).union(condition_6)\n",
    "\n",
    "\n",
    "# Count occurrences of each record and filter records where count >= 4\n",
    "malicious_records = combined_conditions.groupBy(\"prog_code\").agg(sum(\"condition\").alias(\"condition_count\")).filter(col(\"condition_count\") >= 4)\n",
    "\n",
    "# Show the top 150 malicious program entries, ordered by ascending lexicographic order of the prog code\n",
    "malicious_records=malicious_records.orderBy(\"prog_code\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c923831-96c4-4ee6-a7a1-3251eb590785",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>prog_code</th><th>condition_count</th></tr></thead><tbody><tr><td>DVRPGMKEY</td><td>4</td></tr><tr><td>EP000000211576</td><td>4</td></tr><tr><td>EP000000211614</td><td>4</td></tr><tr><td>EP000000211639</td><td>4</td></tr><tr><td>EP000000211645</td><td>4</td></tr><tr><td>EP000000211646</td><td>5</td></tr><tr><td>EP000000211647</td><td>5</td></tr><tr><td>EP000000211648</td><td>5</td></tr><tr><td>EP000000211650</td><td>4</td></tr><tr><td>EP000000211654</td><td>4</td></tr><tr><td>EP000000211659</td><td>4</td></tr><tr><td>EP000000211661</td><td>4</td></tr><tr><td>EP000000211665</td><td>4</td></tr><tr><td>EP000000211666</td><td>4</td></tr><tr><td>EP000000211667</td><td>4</td></tr><tr><td>EP000000211669</td><td>5</td></tr><tr><td>EP000000211670</td><td>4</td></tr><tr><td>EP000000211672</td><td>5</td></tr><tr><td>EP000000211675</td><td>4</td></tr><tr><td>EP000000211676</td><td>4</td></tr><tr><td>EP000000211679</td><td>4</td></tr><tr><td>EP000000211680</td><td>4</td></tr><tr><td>EP000000211681</td><td>4</td></tr><tr><td>EP000000211682</td><td>4</td></tr><tr><td>EP000000211683</td><td>4</td></tr><tr><td>EP000000211684</td><td>4</td></tr><tr><td>EP000000211685</td><td>4</td></tr><tr><td>EP000000211686</td><td>4</td></tr><tr><td>EP000000211688</td><td>4</td></tr><tr><td>EP000000211689</td><td>4</td></tr><tr><td>EP000000211690</td><td>5</td></tr><tr><td>EP000000211691</td><td>4</td></tr><tr><td>EP000000211692</td><td>5</td></tr><tr><td>EP000000211694</td><td>4</td></tr><tr><td>EP000000211696</td><td>4</td></tr><tr><td>EP000000211698</td><td>4</td></tr><tr><td>EP000000260001</td><td>4</td></tr><tr><td>EP000000260036</td><td>4</td></tr><tr><td>EP000000260044</td><td>4</td></tr><tr><td>EP000000260064</td><td>4</td></tr><tr><td>EP000000260115</td><td>4</td></tr><tr><td>EP000000260120</td><td>4</td></tr><tr><td>EP000000351218</td><td>4</td></tr><tr><td>EP000000351219</td><td>4</td></tr><tr><td>EP000000351221</td><td>4</td></tr><tr><td>EP000000351222</td><td>4</td></tr><tr><td>EP000000351227</td><td>4</td></tr><tr><td>EP000000351228</td><td>4</td></tr><tr><td>EP000000351229</td><td>4</td></tr><tr><td>EP000000351230</td><td>4</td></tr><tr><td>EP000000351231</td><td>4</td></tr><tr><td>EP000000351236</td><td>4</td></tr><tr><td>EP000000351237</td><td>4</td></tr><tr><td>EP000000351238</td><td>4</td></tr><tr><td>EP000000351239</td><td>4</td></tr><tr><td>EP000000351240</td><td>4</td></tr><tr><td>EP000000351245</td><td>4</td></tr><tr><td>EP000000351247</td><td>4</td></tr><tr><td>EP000000351250</td><td>4</td></tr><tr><td>EP000000351251</td><td>4</td></tr><tr><td>EP000000351254</td><td>4</td></tr><tr><td>EP000000351255</td><td>4</td></tr><tr><td>EP000000351257</td><td>4</td></tr><tr><td>EP000000351258</td><td>4</td></tr><tr><td>EP000000351260</td><td>4</td></tr><tr><td>EP000000351263</td><td>4</td></tr><tr><td>EP000000351266</td><td>4</td></tr><tr><td>EP000000351269</td><td>4</td></tr><tr><td>EP000000351270</td><td>4</td></tr><tr><td>EP000000351271</td><td>4</td></tr><tr><td>EP000000860002</td><td>4</td></tr><tr><td>EP000000860014</td><td>4</td></tr><tr><td>EP000000860016</td><td>4</td></tr><tr><td>EP000001830003</td><td>4</td></tr><tr><td>EP000001830010</td><td>4</td></tr><tr><td>EP000001830014</td><td>4</td></tr><tr><td>EP000001830019</td><td>4</td></tr><tr><td>EP000001830024</td><td>4</td></tr><tr><td>EP000001830031</td><td>4</td></tr><tr><td>EP000001830036</td><td>4</td></tr><tr><td>EP000001830045</td><td>4</td></tr><tr><td>EP000001830050</td><td>4</td></tr><tr><td>EP000001830052</td><td>4</td></tr><tr><td>EP000001830054</td><td>4</td></tr><tr><td>EP000001830058</td><td>4</td></tr><tr><td>EP000001830060</td><td>4</td></tr><tr><td>EP000001830062</td><td>4</td></tr><tr><td>EP000001830064</td><td>4</td></tr><tr><td>EP000001830067</td><td>4</td></tr><tr><td>EP000001830072</td><td>4</td></tr><tr><td>EP000001830077</td><td>4</td></tr><tr><td>EP000001830087</td><td>4</td></tr><tr><td>EP000001830091</td><td>4</td></tr><tr><td>EP000002040167</td><td>4</td></tr><tr><td>EP000002040181</td><td>4</td></tr><tr><td>EP000002360021</td><td>4</td></tr><tr><td>EP000002880131</td><td>4</td></tr><tr><td>EP000002880211</td><td>4</td></tr><tr><td>EP000002880232</td><td>4</td></tr><tr><td>EP000002880233</td><td>4</td></tr><tr><td>EP000002880235</td><td>4</td></tr><tr><td>EP000002880236</td><td>4</td></tr><tr><td>EP000002880241</td><td>4</td></tr><tr><td>EP000002880242</td><td>4</td></tr><tr><td>EP000003240004</td><td>4</td></tr><tr><td>EP000003240005</td><td>4</td></tr><tr><td>EP000003240008</td><td>4</td></tr><tr><td>EP000003240009</td><td>4</td></tr><tr><td>EP000003240010</td><td>4</td></tr><tr><td>EP000003240013</td><td>4</td></tr><tr><td>EP000003240014</td><td>4</td></tr><tr><td>EP000003240015</td><td>4</td></tr><tr><td>EP000003240016</td><td>4</td></tr><tr><td>EP000003240018</td><td>4</td></tr><tr><td>EP000003240021</td><td>4</td></tr><tr><td>EP000003240023</td><td>4</td></tr><tr><td>EP000003240025</td><td>4</td></tr><tr><td>EP000003240027</td><td>4</td></tr><tr><td>EP000003240028</td><td>4</td></tr><tr><td>EP000003240029</td><td>4</td></tr><tr><td>EP000003240032</td><td>4</td></tr><tr><td>EP000003240033</td><td>4</td></tr><tr><td>EP000003240035</td><td>4</td></tr><tr><td>EP000003240036</td><td>4</td></tr><tr><td>EP000003240038</td><td>4</td></tr><tr><td>EP000003240041</td><td>4</td></tr><tr><td>EP000003240042</td><td>4</td></tr><tr><td>EP000003240043</td><td>4</td></tr><tr><td>EP000003240044</td><td>4</td></tr><tr><td>EP000003240045</td><td>4</td></tr><tr><td>EP000003240046</td><td>4</td></tr><tr><td>EP000003240047</td><td>4</td></tr><tr><td>EP000003240048</td><td>4</td></tr><tr><td>EP000003240050</td><td>4</td></tr><tr><td>EP000003240051</td><td>4</td></tr><tr><td>EP000003240052</td><td>4</td></tr><tr><td>EP000003240055</td><td>4</td></tr><tr><td>EP000003240056</td><td>4</td></tr><tr><td>EP000003240057</td><td>4</td></tr><tr><td>EP000003240058</td><td>4</td></tr><tr><td>EP000003240059</td><td>4</td></tr><tr><td>EP000003240060</td><td>4</td></tr><tr><td>EP000003240061</td><td>4</td></tr><tr><td>EP000003240062</td><td>4</td></tr><tr><td>EP000003240064</td><td>4</td></tr><tr><td>EP000003240066</td><td>4</td></tr><tr><td>EP000003240067</td><td>4</td></tr><tr><td>EP000003240069</td><td>4</td></tr><tr><td>EP000003240077</td><td>4</td></tr><tr><td>EP000003240081</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DVRPGMKEY",
         4
        ],
        [
         "EP000000211576",
         4
        ],
        [
         "EP000000211614",
         4
        ],
        [
         "EP000000211639",
         4
        ],
        [
         "EP000000211645",
         4
        ],
        [
         "EP000000211646",
         5
        ],
        [
         "EP000000211647",
         5
        ],
        [
         "EP000000211648",
         5
        ],
        [
         "EP000000211650",
         4
        ],
        [
         "EP000000211654",
         4
        ],
        [
         "EP000000211659",
         4
        ],
        [
         "EP000000211661",
         4
        ],
        [
         "EP000000211665",
         4
        ],
        [
         "EP000000211666",
         4
        ],
        [
         "EP000000211667",
         4
        ],
        [
         "EP000000211669",
         5
        ],
        [
         "EP000000211670",
         4
        ],
        [
         "EP000000211672",
         5
        ],
        [
         "EP000000211675",
         4
        ],
        [
         "EP000000211676",
         4
        ],
        [
         "EP000000211679",
         4
        ],
        [
         "EP000000211680",
         4
        ],
        [
         "EP000000211681",
         4
        ],
        [
         "EP000000211682",
         4
        ],
        [
         "EP000000211683",
         4
        ],
        [
         "EP000000211684",
         4
        ],
        [
         "EP000000211685",
         4
        ],
        [
         "EP000000211686",
         4
        ],
        [
         "EP000000211688",
         4
        ],
        [
         "EP000000211689",
         4
        ],
        [
         "EP000000211690",
         5
        ],
        [
         "EP000000211691",
         4
        ],
        [
         "EP000000211692",
         5
        ],
        [
         "EP000000211694",
         4
        ],
        [
         "EP000000211696",
         4
        ],
        [
         "EP000000211698",
         4
        ],
        [
         "EP000000260001",
         4
        ],
        [
         "EP000000260036",
         4
        ],
        [
         "EP000000260044",
         4
        ],
        [
         "EP000000260064",
         4
        ],
        [
         "EP000000260115",
         4
        ],
        [
         "EP000000260120",
         4
        ],
        [
         "EP000000351218",
         4
        ],
        [
         "EP000000351219",
         4
        ],
        [
         "EP000000351221",
         4
        ],
        [
         "EP000000351222",
         4
        ],
        [
         "EP000000351227",
         4
        ],
        [
         "EP000000351228",
         4
        ],
        [
         "EP000000351229",
         4
        ],
        [
         "EP000000351230",
         4
        ],
        [
         "EP000000351231",
         4
        ],
        [
         "EP000000351236",
         4
        ],
        [
         "EP000000351237",
         4
        ],
        [
         "EP000000351238",
         4
        ],
        [
         "EP000000351239",
         4
        ],
        [
         "EP000000351240",
         4
        ],
        [
         "EP000000351245",
         4
        ],
        [
         "EP000000351247",
         4
        ],
        [
         "EP000000351250",
         4
        ],
        [
         "EP000000351251",
         4
        ],
        [
         "EP000000351254",
         4
        ],
        [
         "EP000000351255",
         4
        ],
        [
         "EP000000351257",
         4
        ],
        [
         "EP000000351258",
         4
        ],
        [
         "EP000000351260",
         4
        ],
        [
         "EP000000351263",
         4
        ],
        [
         "EP000000351266",
         4
        ],
        [
         "EP000000351269",
         4
        ],
        [
         "EP000000351270",
         4
        ],
        [
         "EP000000351271",
         4
        ],
        [
         "EP000000860002",
         4
        ],
        [
         "EP000000860014",
         4
        ],
        [
         "EP000000860016",
         4
        ],
        [
         "EP000001830003",
         4
        ],
        [
         "EP000001830010",
         4
        ],
        [
         "EP000001830014",
         4
        ],
        [
         "EP000001830019",
         4
        ],
        [
         "EP000001830024",
         4
        ],
        [
         "EP000001830031",
         4
        ],
        [
         "EP000001830036",
         4
        ],
        [
         "EP000001830045",
         4
        ],
        [
         "EP000001830050",
         4
        ],
        [
         "EP000001830052",
         4
        ],
        [
         "EP000001830054",
         4
        ],
        [
         "EP000001830058",
         4
        ],
        [
         "EP000001830060",
         4
        ],
        [
         "EP000001830062",
         4
        ],
        [
         "EP000001830064",
         4
        ],
        [
         "EP000001830067",
         4
        ],
        [
         "EP000001830072",
         4
        ],
        [
         "EP000001830077",
         4
        ],
        [
         "EP000001830087",
         4
        ],
        [
         "EP000001830091",
         4
        ],
        [
         "EP000002040167",
         4
        ],
        [
         "EP000002040181",
         4
        ],
        [
         "EP000002360021",
         4
        ],
        [
         "EP000002880131",
         4
        ],
        [
         "EP000002880211",
         4
        ],
        [
         "EP000002880232",
         4
        ],
        [
         "EP000002880233",
         4
        ],
        [
         "EP000002880235",
         4
        ],
        [
         "EP000002880236",
         4
        ],
        [
         "EP000002880241",
         4
        ],
        [
         "EP000002880242",
         4
        ],
        [
         "EP000003240004",
         4
        ],
        [
         "EP000003240005",
         4
        ],
        [
         "EP000003240008",
         4
        ],
        [
         "EP000003240009",
         4
        ],
        [
         "EP000003240010",
         4
        ],
        [
         "EP000003240013",
         4
        ],
        [
         "EP000003240014",
         4
        ],
        [
         "EP000003240015",
         4
        ],
        [
         "EP000003240016",
         4
        ],
        [
         "EP000003240018",
         4
        ],
        [
         "EP000003240021",
         4
        ],
        [
         "EP000003240023",
         4
        ],
        [
         "EP000003240025",
         4
        ],
        [
         "EP000003240027",
         4
        ],
        [
         "EP000003240028",
         4
        ],
        [
         "EP000003240029",
         4
        ],
        [
         "EP000003240032",
         4
        ],
        [
         "EP000003240033",
         4
        ],
        [
         "EP000003240035",
         4
        ],
        [
         "EP000003240036",
         4
        ],
        [
         "EP000003240038",
         4
        ],
        [
         "EP000003240041",
         4
        ],
        [
         "EP000003240042",
         4
        ],
        [
         "EP000003240043",
         4
        ],
        [
         "EP000003240044",
         4
        ],
        [
         "EP000003240045",
         4
        ],
        [
         "EP000003240046",
         4
        ],
        [
         "EP000003240047",
         4
        ],
        [
         "EP000003240048",
         4
        ],
        [
         "EP000003240050",
         4
        ],
        [
         "EP000003240051",
         4
        ],
        [
         "EP000003240052",
         4
        ],
        [
         "EP000003240055",
         4
        ],
        [
         "EP000003240056",
         4
        ],
        [
         "EP000003240057",
         4
        ],
        [
         "EP000003240058",
         4
        ],
        [
         "EP000003240059",
         4
        ],
        [
         "EP000003240060",
         4
        ],
        [
         "EP000003240061",
         4
        ],
        [
         "EP000003240062",
         4
        ],
        [
         "EP000003240064",
         4
        ],
        [
         "EP000003240066",
         4
        ],
        [
         "EP000003240067",
         4
        ],
        [
         "EP000003240069",
         4
        ],
        [
         "EP000003240077",
         4
        ],
        [
         "EP000003240081",
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "prog_code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "condition_count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(malicious_records.limit(150))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project1_part1_207796996_319044434",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
